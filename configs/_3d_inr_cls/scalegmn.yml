batch_size: 32
data:
  dataset: labeled_3d_inr
  dataset_path: ./data/nfn_1x128_1k_rwi_pth
  node_pos_embed: &node_pos_embed True #TODO: Change to True
  edge_pos_embed: &edge_pos_embed False  #TODO
  layer_layout: [3, 128, 1]
  switch_to_canon: False

train_args:
  num_epochs: 400 #200
  val_acc_threshold: 0.1
  patience: 30 #50
  seed: 0
  loss: CrossEntropyLoss

scalegmn_args:
  d_in_v: &d_in_v 1  # initial dimension of input nn bias
  d_in_e: &d_in_e 1  # initial dimension of input nn weights
  d_hid: &d_hid 64 #64 hidden dimension
  num_layers: 4 # number of gnn layers to apply
  direction: forward
  equivariant: False
  symmetry: sign  # sign or permutation
  jit: False # prefer compile - compile gnn to optimize performance
  compile: True # False # compile gnn to optimize performance

  readout_range: full_graph #last_layer  # or full_graph # last_layer doesn't work with subsampling
  gnn_skip_connections: True # False

  concat_mlp_directions: False
  reciprocal: True

  node_pos_embed: *node_pos_embed  # use positional encodings
  edge_pos_embed: *edge_pos_embed  # use positional encodings

  graph_init:
    d_in_v: *d_in_v
    d_in_e: *d_in_e
    project_node_feats: True
    project_edge_feats: True
    d_node: *d_hid
    d_edge: *d_hid
    

  positional_encodings:
    final_linear_pos_embed: False
    sum_pos_enc: False
    po_as_different_linear: False
    equiv_net: False
    # args for the equiv net option.
    sum_on_io: True
    equiv_on_hidden: True
    num_mlps: 3
    layer_equiv_on_hidden: False

  gnn_args:
    d_hid: *d_hid
    message_fn_layers: 1
    message_fn_skip_connections: False
    update_node_feats_fn_layers: 1
    update_node_feats_fn_skip_connections: False
    update_edge_attr: True
    dropout: 0.01 # 0.2
    dropout_all: True # False  # False: only in between the gnn layers, True: + all mlp layers
    update_as_act: False
    update_as_act_arg: sum
    mlp_on_io: True

    msg_equiv_on_hidden: True
    upd_equiv_on_hidden: True
    layer_msg_equiv_on_hidden: False
    layer_upd_equiv_on_hidden: False
    msg_num_mlps: 3
    upd_num_mlps: 3
    pos_embed_msg: False
    pos_embed_upd: False
    layer_norm: True # False
    aggregator: add
    sign_symmetrization: False

  mlp_args:
    d_k: [ *d_hid ]
    activation: silu
    dropout: 0.
    final_activation: identity
    batch_norm: False
    layer_norm: True
    bias: True
    skip: False # True Error: The readout MLP in ScaleGMN is set up to go from (d_in = 4) â†’ (d_out = 10)

  readout_args:
    d_out: 10  #10 # final dimension of readout Permutation Invariant Scale(Sign)Net - Number of Classes for classification tasks.
    d_rho: *d_hid  # intermediate dimension within Readout module


optimization:
  clip_grad: True
  clip_grad_max_norm: 10.0
  optimizer_name: AdamW
  optimizer_args:
    lr: 0.0002
    weight_decay: 0.00001
  scheduler_args:
    scheduler: WarmupLRScheduler
    warmup_steps: 1000
    scheduler_mode: min
    decay_rate: 0
    decay_steps: 0
    patience: None
    min_lr: None

wandb_args:
  project: _3d_inr_cls
  entity: null
  group: null
  name: null
  tags: null
  notes: # "INRs nfn_1x128_1k_rwi_pth, Without Phase Canonicalization, scalegmn with sign"